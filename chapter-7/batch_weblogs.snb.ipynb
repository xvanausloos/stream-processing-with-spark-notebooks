{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A7ECD7E2EBF047A283DBADDCA0BB55E1"
   },
   "source": [
    "# Simple Weblog Analytics - The Batch Way\n",
    "In this notebook, we are going to quickly visit a batch process of a series of weblog files to obtain the top trending pages per day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "4D776A6E557446488681874D1D710705",
    "input_collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "logsDirectory: String = /home/ldi/nasa_dataset_july_1995\n",
       "rawLogs: org.apache.spark.sql.DataFrame = [bytes: bigint, host: string ... 3 more fields]\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// This is the location of the unpackaged files. Update accordingly\n",
    "// You can unpack the provided dataset with:\n",
    "// tar xvf datasets/NASA-weblogs/nasa_dataset_july_1995.tgz -C /tmp/data/\n",
    "val logsDirectory = \"/home/ldi/nasa_dataset_july_1995\"\n",
    "val rawLogs = spark.read.json(logsDirectory)\n",
    "\n",
    "// test xav"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A60148A17B904A72A421640BB8E87474"
   },
   "source": [
    "## We define a schema for the data in the logs\n",
    "Following the formal description of the dataset (at: [NASA-HTTP](http://ita.ee.lbl.gov/html/contrib/NASA-HTTP.html) ), the log is structured as follows:\n",
    "\n",
    ">The logs are an ASCII file with one line per request, with the following columns:\n",
    "- host making the request. A hostname when possible, otherwise the Internet address if the name could not be looked up.\n",
    "- timestamp in the format \"DAY MON DD HH:MM:SS YYYY\", where DAY is the day of the week, MON is the name of the month, DD is the day of the month, HH:MM:SS is the time of day using a 24-hour clock, and YYYY is the year. The timezone is -0400.\n",
    "- request given in quotes.\n",
    "- HTTP reply code.\n",
    "- bytes in the reply.\n",
    "\n",
    "The dataset provided for this exercise offers this data in JSON format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "82F9E2879A1747EF8CC36B5E5D64E701",
    "input_collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import java.sql.Timestamp\n",
       "defined class WebLog\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import java.sql.Timestamp\n",
    "case class WebLog(host:String, \n",
    "                  timestamp: String, \n",
    "                  request: String, \n",
    "                  http_reply:Int, \n",
    "                  bytes: Long\n",
    "                 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CB0651C3AB3747EF9CB38477A6E59201"
   },
   "source": [
    "## We convert the raw data to structured logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "7AF0F566B58C49F195A3D1E4990BA3E6",
    "input_collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import java.time.ZonedDateTime\n",
       "import java.time.format.DateTimeFormatter\n",
       "import java.sql.Timestamp\n",
       "import org.apache.spark.sql.functions._\n",
       "import org.apache.spark.sql.types.IntegerType\n",
       "preparedLogs: org.apache.spark.sql.DataFrame = [bytes: bigint, host: string ... 3 more fields]\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import java.time.ZonedDateTime\n",
    "import java.time.format.DateTimeFormatter\n",
    "import java.sql.Timestamp\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.types.IntegerType\n",
    "val preparedLogs = rawLogs.withColumn(\"http_reply\", $\"http_reply\".cast(IntegerType))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "833438CD8CA1471181AC509553B749C2",
    "input_collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------------+----------+--------------------+--------------------+\n",
      "|  bytes|            host|http_reply|             request|           timestamp|\n",
      "+-------+----------------+----------+--------------------+--------------------+\n",
      "|      0| nntp1.reach.com|       304|GET /images/NASA-...|1995-07-13T00:00:...|\n",
      "|  32252|webgate1.mot.com|       200|GET /shuttle/tech...|1995-07-13T00:00:...|\n",
      "|    751|webgate1.mot.com|       200|GET /htbin/cdt_cl...|1995-07-13T00:00:...|\n",
      "|1121554|  204.157.128.52|       200|GET /shuttle/miss...|1995-07-13T00:00:...|\n",
      "|      0| nntp1.reach.com|       304|GET /images/KSC-l...|1995-07-13T00:00:...|\n",
      "+-------+----------------+----------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "res0: String =\n",
       "\"root\n",
       " |-- bytes: long (nullable = true)\n",
       " |-- host: string (nullable = true)\n",
       " |-- http_reply: integer (nullable = true)\n",
       " |-- request: string (nullable = true)\n",
       " |-- timestamp: string (nullable = true)\n",
       "\"\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preparedLogs.show(5)\n",
    "preparedLogs.schema.treeString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "weblogs: org.apache.spark.sql.Dataset[WebLog] = [bytes: bigint, host: string ... 3 more fields]\n",
       "res1: String =\n",
       "\"root\n",
       " |-- bytes: long (nullable = true)\n",
       " |-- host: string (nullable = true)\n",
       " |-- http_reply: integer (nullable = true)\n",
       " |-- request: string (nullable = true)\n",
       " |-- timestamp: string (nullable = true)\n",
       "\"\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val weblogs = preparedLogs.as[WebLog] //convert DF preparedLogs to a dataset of given type (Weblog)\n",
    "weblogs.schema.treeString"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "9313FDE7CE7548B180AE2A47CDBED56E",
    "input_collapsed": false
   },
   "source": [
    "## Now, we have the data in a structured format and we can start asking the questions that interest us.\n",
    "We have imported the data and transformed it using a known schema.  We can use this 'structured' data to create queries that provide insights in the behavior of the users. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "144FB0F8C3AE42328435A2A8F3C150AB"
   },
   "source": [
    "### As a first step, we would like to know how many records are contained in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "FBE9E30417204C9D83E8F1FE65AB6471",
    "input_collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "recordCount: Long = 1871988\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    val recordCount = weblogs.count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BEA0D41E58324A479D2B91093864E6C7"
   },
   "source": [
    "### A common question would be, what was the most popular URL per day?\n",
    "We first reduce the timestamp to the day of the year. We then group by this new 'day of year' column and the request url and we count over this aggregate. We finally order using descending order to get this top URLs first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------------+----------+--------------------+--------------------+----------+\n",
      "|  bytes|            host|http_reply|             request|           timestamp|dayOfMonth|\n",
      "+-------+----------------+----------+--------------------+--------------------+----------+\n",
      "|      0| nntp1.reach.com|       304|GET /images/NASA-...|1995-07-13T00:00:...|        13|\n",
      "|  32252|webgate1.mot.com|       200|GET /shuttle/tech...|1995-07-13T00:00:...|        13|\n",
      "|    751|webgate1.mot.com|       200|GET /htbin/cdt_cl...|1995-07-13T00:00:...|        13|\n",
      "|1121554|  204.157.128.52|       200|GET /shuttle/miss...|1995-07-13T00:00:...|        13|\n",
      "|      0| nntp1.reach.com|       304|GET /images/KSC-l...|1995-07-13T00:00:...|        13|\n",
      "+-------+----------------+----------+--------------------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "topDailyURLsStep1: org.apache.spark.sql.DataFrame = [bytes: bigint, host: string ... 4 more fields]\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val topDailyURLsStep1 = weblogs.withColumn(\"dayOfMonth\", dayofmonth($\"timestamp\"))\n",
    "topDailyURLsStep1.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+-----+\n",
      "|dayOfMonth|             request|count|\n",
      "+----------+--------------------+-----+\n",
      "|        13|GET /icons/sound....|   98|\n",
      "|        13|GET /cgi-bin/imag...|    2|\n",
      "|        13|GET /cgi-bin/imag...|    2|\n",
      "|        13|GET /htbin/wais.p...|   14|\n",
      "|        13|GET /statistics/1...|    1|\n",
      "+----------+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "topDailyURLsStep2: org.apache.spark.sql.DataFrame = [dayOfMonth: int, request: string ... 1 more field]\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val topDailyURLsStep2 = topDailyURLsStep1\n",
    "    .select($\"request\",$\"dayOfMonth\")\n",
    "    .groupBy($\"dayOfMonth\", $\"request\")\n",
    "    .agg(count($\"request\").alias(\"count\"))\n",
    "topDailyURLsStep2.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "B65C7756F4EA47388884AB0DC4619CE8",
    "input_collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "topDailyURLs: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [dayOfMonth: int, request: string ... 1 more field]\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val topDailyURLs = weblogs.withColumn(\"dayOfMonth\", dayofmonth($\"timestamp\"))\n",
    "                          .select($\"request\", $\"dayOfMonth\")\n",
    "                          .groupBy($\"dayOfMonth\", $\"request\")\n",
    "                          .agg(count($\"request\").alias(\"count\"))\n",
    "                          .orderBy(desc(\"count\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "F1CB4530E62149F08D39256D53BA1C07",
    "input_collapsed": false,
    "presentation": {
     "pivot_chart_state": "{\n  \"hiddenAttributes\": [],\n  \"menuLimit\": 200,\n  \"cols\": [],\n  \"rows\": [],\n  \"vals\": [],\n  \"exclusions\": {},\n  \"inclusions\": {},\n  \"unusedAttrsVertical\": 85,\n  \"autoSortUnusedAttrs\": false,\n  \"inclusionsInfo\": {},\n  \"aggregatorName\": \"Count\",\n  \"rendererName\": \"Table\"\n}",
     "tabs_state": "{\n  \"tab_id\": \"#tab2096092733-0\"\n}"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res4: Array[org.apache.spark.sql.Row] = Array([13,GET /images/NASA-logosmall.gif HTTP/1.0,12476], [13,GET /htbin/cdt_main.pl HTTP/1.0,7471], [12,GET /images/NASA-logosmall.gif HTTP/1.0,7143], [13,GET /htbin/cdt_clock.pl HTTP/1.0,6237], [6,GET /images/NASA-logosmall.gif HTTP/1.0,6112], [5,GET /images/NASA-logosmall.gif HTTP/1.0,5865], [13,GET /images/KSC-logosmall.gif HTTP/1.0,5662], [7,GET /images/NASA-logosmall.gif HTTP/1.0,5651], [3,GET /images/NASA-logosmall.gif HTTP/1.0,5356], [6,GET /images/KSC-logosmall.gif HTTP/1.0,5126])\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topDailyURLs.take(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+-----+\n",
      "|dayOfMonth|             request|count|\n",
      "+----------+--------------------+-----+\n",
      "|        13|GET /images/NASA-...|12476|\n",
      "|        13|GET /htbin/cdt_ma...| 7471|\n",
      "|        12|GET /images/NASA-...| 7143|\n",
      "|        13|GET /htbin/cdt_cl...| 6237|\n",
      "|         6|GET /images/NASA-...| 6112|\n",
      "|         5|GET /images/NASA-...| 5865|\n",
      "|        13|GET /images/KSC-l...| 5662|\n",
      "|         7|GET /images/NASA-...| 5651|\n",
      "|         3|GET /images/NASA-...| 5356|\n",
      "|         6|GET /images/KSC-l...| 5126|\n",
      "+----------+--------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "topDailyURLs.show(10) //show allows better table view."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4F7AB40ADF1D4EFF9166CDB63A0DC77E"
   },
   "source": [
    "### Top hits are all images. What now?\n",
    "It's not unusual to see that the top URLs are images commonly used across a site.\n",
    "\n",
    "Our true interest lies in the content pages generating most traffic. To find those, we first filter on `html` content \n",
    "and then proceed to apply the top aggregation we just learned.\n",
    "\n",
    "The request field is a quoted sequence of `[HTTP_VERB] URL [HTTP_VERSION]`. We will extract the url and preserve only those ending in `.html`, `.htm` or no extension (directories). This is a simplification for the purpose of the exercise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+----------+--------------------+--------------------+\n",
      "|  bytes|                host|http_reply|             request|           timestamp|\n",
      "+-------+--------------------+----------+--------------------+--------------------+\n",
      "|      0|     nntp1.reach.com|       304|GET /images/NASA-...|1995-07-13T00:00:...|\n",
      "|  32252|    webgate1.mot.com|       200|GET /shuttle/tech...|1995-07-13T00:00:...|\n",
      "|    751|    webgate1.mot.com|       200|GET /htbin/cdt_cl...|1995-07-13T00:00:...|\n",
      "|1121554|      204.157.128.52|       200|GET /shuttle/miss...|1995-07-13T00:00:...|\n",
      "|      0|     nntp1.reach.com|       304|GET /images/KSC-l...|1995-07-13T00:00:...|\n",
      "|      0|    webgate1.mot.com|       304|GET /images/NASA-...|1995-07-13T00:00:...|\n",
      "| 190269|    webgate1.mot.com|       200|GET /shuttle/miss...|1995-07-13T00:00:...|\n",
      "|   7008|    darkstar.isi.edu|       200|GET /facilities/l...|1995-07-13T00:00:...|\n",
      "|  57344|dd08-048.compuser...|       200|GET /shuttle/coun...|1995-07-13T00:00:...|\n",
      "|  13116|    darkstar.isi.edu|       200|GET /images/lc39a...|1995-07-13T00:00:...|\n",
      "+-------+--------------------+----------+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "weblogs.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "7191956637C34DF28D24D2947E6999A8",
    "input_collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "urlExtractor: scala.util.matching.Regex = ^GET (.+) HTTP/\\d.\\d\n",
       "allowedExtensions: scala.collection.immutable.Set[String] = Set(.html, .htm, \"\")\n",
       "contentPageLogs: org.apache.spark.sql.Dataset[WebLog] = [bytes: bigint, host: string ... 3 more fields]\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val urlExtractor = \"\"\"^GET (.+) HTTP/\\d.\\d\"\"\".r\n",
    "val allowedExtensions = Set(\".html\",\".htm\", \"\")\n",
    "val contentPageLogs = weblogs.filter {log => \n",
    "  log.request match {                                        \n",
    "    case urlExtractor(url) => \n",
    "      val ext = url.takeRight(5).dropWhile(c => c != '.')\n",
    "      allowedExtensions.contains(ext)\n",
    "    case _ => false \n",
    "  }\n",
    "}\n",
    "                                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "org.apache.spark.SparkException",
     "evalue": " Job aborted due to stage failure: Task 0 in stage 17.0 failed 1 times, most recent failure: Lost task 0.0 in stage 17.0 (TID 31) (ubuntu20ldi1 executor driver): java.lang.ClassCastException: $iw cannot be cast to $iw",
     "output_type": "error",
     "traceback": [
      "org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 17.0 failed 1 times, most recent failure: Lost task 0.0 in stage 17.0 (TID 31) (ubuntu20ldi1 executor driver): java.lang.ClassCastException: $iw cannot be cast to $iw",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:350)",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1491)",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)",
      "\tat java.lang.Thread.run(Thread.java:750)",
      "",
      "Driver stacktrace:",
      "  at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)",
      "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)",
      "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)",
      "  at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)",
      "  at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)",
      "  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)",
      "  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)",
      "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)",
      "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)",
      "  at scala.Option.foreach(Option.scala:407)",
      "  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)",
      "  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)",
      "  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2235)",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2254)",
      "  at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:492)",
      "  at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:445)",
      "  at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:48)",
      "  at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3715)",
      "  at org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2728)",
      "  at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3706)",
      "  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)",
      "  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)",
      "  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)",
      "  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)",
      "  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)",
      "  at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3704)",
      "  at org.apache.spark.sql.Dataset.head(Dataset.scala:2728)",
      "  at org.apache.spark.sql.Dataset.take(Dataset.scala:2935)",
      "  at org.apache.spark.sql.Dataset.getRows(Dataset.scala:287)",
      "  at org.apache.spark.sql.Dataset.showString(Dataset.scala:326)",
      "  at org.apache.spark.sql.Dataset.show(Dataset.scala:806)",
      "  at org.apache.spark.sql.Dataset.show(Dataset.scala:765)",
      "  ... 41 elided",
      "Caused by: java.lang.ClassCastException: $iw cannot be cast to $iw",
      "  at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)",
      "  at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)",
      "  at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)",
      "  at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:350)",
      "  at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)",
      "  at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)",
      "  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)",
      "  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)",
      "  at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)",
      "  at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)",
      "  at org.apache.spark.scheduler.Task.run(Task.scala:131)",
      "  at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)",
      "  at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1491)",
      "  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)",
      "  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)",
      "  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)",
      "  ... 1 more",
      ""
     ]
    }
   ],
   "source": [
    "contentPageLogs.show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "844604F254BF4450B2D60F09A766C444",
    "input_collapsed": false,
    "presentation": {
     "pivot_chart_state": "{\n  \"hiddenAttributes\": [],\n  \"menuLimit\": 200,\n  \"cols\": [],\n  \"rows\": [],\n  \"vals\": [],\n  \"exclusions\": {},\n  \"inclusions\": {},\n  \"unusedAttrsVertical\": 85,\n  \"autoSortUnusedAttrs\": false,\n  \"inclusionsInfo\": {},\n  \"aggregatorName\": \"Count\",\n  \"rendererName\": \"Table\"\n}",
     "tabs_state": "{\n  \"tab_id\": \"#tab1029193174-0\"\n}"
    }
   },
   "outputs": [],
   "source": [
    "val topContentPages = contentPageLogs.withColumn(\"dayOfMonth\", dayofmonth($\"timestamp\"))\n",
    "                          .select($\"request\", $\"dayOfMonth\")\n",
    "                          .groupBy($\"dayOfMonth\", $\"request\")\n",
    "                          .agg(count($\"request\").alias(\"count\"))\n",
    "                          .orderBy(desc(\"count\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "392A9C52AEC34D9F8EF709F5B407F0E1",
    "input_collapsed": false
   },
   "outputs": [],
   "source": [
    "topContentPages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "995C97DC2E9F4DBA95A832588B147843"
   },
   "source": [
    "We can see that the most popular page that month was `liftoff.html ` corresponding to the coverage of the launch of the Discovery shuttle, as documented on the NASA archives: https://www.nasa.gov/mission_pages/shuttle/shuttlemissions/archives/sts-70.html.\n",
    "\n",
    "It's closely followed by `countdown/` the days prior ot the launch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8DBF5B380B654300B6064149A1B63ABC",
    "input_collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "auto_save_timestamp": "1970-01-01T01:00:00.000Z",
  "customArgs": null,
  "customDeps": null,
  "customImports": null,
  "customLocalRepo": null,
  "customRepos": null,
  "customSparkConf": null,
  "customVars": null,
  "id": "c8749075-25ae-48aa-a102-495a26a92d54",
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  },
  "name": "batch_weblogs",
  "sparkNotebook": null,
  "trusted": true,
  "user_save_timestamp": "1970-01-01T01:00:00.000Z"
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
