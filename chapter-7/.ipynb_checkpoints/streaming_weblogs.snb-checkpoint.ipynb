{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "A2D4928A0946433492E0431FB9AD1D66",
    "input_collapsed": false
   },
   "source": [
    "# Simple Weblog Analytics - The Streaming Way\n",
    "In this notebook, we are going to explore the weblog use case using the stream 'as it happens'.\n",
    "\n",
    "This notebook requires a local `TCP ` server that simulates the Web server sending data.  \n",
    "\n",
    "Please start the [weblogs_TCP_Server](./weblogs_TCP_server.snb.ipynb) notebook before running this one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "66DD67FAD0C24E00965CD26C0858D46B"
   },
   "source": [
    "## To connect to a TCP source, we need the host and the port of the TCP server.\n",
    "Here we use the defaults used in the `weblogs_TCP_server` notebook. If you changed these parameters there, change them here accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "35F22D17E25940618FFE838C99405B16",
    "input_collapsed": false
   },
   "outputs": [],
   "source": [
    "val host = \"localhost\"\n",
    "val port = 9999"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C0E80D55C81340318BD2CD304E322F5B"
   },
   "source": [
    "## We use the `TextSocketSource` in Structured Streaming to connect to the TCP server and consume the text stream.\n",
    "This `Source` is called `socket` as the short name we can use as `format` to instantiate it.\n",
    "\n",
    "The options needed to configure the `socket` `Source` are `host` and `port` to provide the configuration of our TCP server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1356EE7F9684454E89FA1F1F8D5CAA95",
    "input_collapsed": false
   },
   "outputs": [],
   "source": [
    "val stream = sparkSession.readStream\n",
    "  .format(\"socket\")\n",
    "  .option(\"host\", host)\n",
    "  .option(\"port\", port)\n",
    "  .load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "87702285726047CD8F3BE8FE0471C863"
   },
   "source": [
    "## We define a schema for the data in the logs\n",
    "Following the formal description of the dataset (at: [NASA-HTTP](http://ita.ee.lbl.gov/html/contrib/NASA-HTTP.html) ), the log is structured as follows:\n",
    "\n",
    ">The logs are an ASCII file with one line per request, with the following columns:\n",
    "- host making the request. A hostname when possible, otherwise the Internet address if the name could not be looked up.\n",
    "- timestamp in the format \"DAY MON DD HH:MM:SS YYYY\", where DAY is the day of the week, MON is the name of the month, DD is the day of the month, HH:MM:SS is the time of day using a 24-hour clock, and YYYY is the year. The timezone is -0400.\n",
    "- request given in quotes.\n",
    "- HTTP reply code.\n",
    "- bytes in the reply.\n",
    "\n",
    "The dataset provided for this exercise offers this data in JSON format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "727B6082C6C24A39B0D651583E62BFBC",
    "input_collapsed": false
   },
   "outputs": [],
   "source": [
    "import java.sql.Timestamp\n",
    "case class WebLog(host:String, \n",
    "                  timestamp: Timestamp, \n",
    "                  request: String, \n",
    "                  http_reply:Int, \n",
    "                  bytes: Long\n",
    "                 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "52B745BDE1A44B928CF66DB4118589A4"
   },
   "source": [
    "##We convert the raw data to structured logs\n",
    "In the batch analytics case we could load the data directly as JSON records. In the case of the `Socket` source, that data is received as plain text.\n",
    "To transform our raw data to `WebLog` records, we first require a schema. The schema provides the necessary information to parse the text to a JSON object. It's the 'structure' when we talk about 'structured'  streaming.\n",
    "\n",
    "After defining a schema for our data, we will:\n",
    "\n",
    "- Transform the text `value` to JSON using the JSON support built in the structured API of Spark\n",
    "- Use the `Dataset` API to transform the JSON records to `WebLog` objects\n",
    "\n",
    "As result of this process, we will obtain a `Streaming Dataset` of `WebLog` records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DADD984537CC4AE3816E2BA42480F969",
    "input_collapsed": false
   },
   "outputs": [],
   "source": [
    "val webLogSchema = Encoders.product[WebLog].schema "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C39C1D8FE59242F58953E8F72E95E2AD",
    "input_collapsed": false
   },
   "outputs": [],
   "source": [
    "val jsonStream = stream.select(from_json($\"value\", webLogSchema) as \"record\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "62E7E2F5CDC04C1185FD779141748500",
    "input_collapsed": false
   },
   "outputs": [],
   "source": [
    "val webLogStream: Dataset[WebLog] = jsonStream.select(\"record.*\").as[WebLog]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7BF49320BEDD4896882407FDF6CD3E37"
   },
   "source": [
    "## We have a structured stream.\n",
    "The `webLogStream` we just obtained is of type `Dataset[WebLog]` like we had in the batch analytics job.\n",
    "The difference between this instance and the batch version is that `webLogStream` is a streaming `Dataset`.\n",
    "\n",
    "We can observe this by querying the object.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E18B9BF47F9644EE8EC52885827EF4EE",
    "input_collapsed": false
   },
   "outputs": [],
   "source": [
    "webLogStream.isStreaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "41E1A019CAEE4976B484B8A5D59D100E"
   },
   "source": [
    "## Operations on Streaming Datasets\n",
    "At this point in the batch job, we were creating the first query on our data: How many records are contained in our dataset?\n",
    "This is a question that we can answer easily when we have access to all the data. But how to count records that are constantly arriving? \n",
    "The answer is that some operations we consider usual on a static `Dataset`, like counting all records, do not have a defined meaning on a streaming Dataset.\n",
    "\n",
    "As we can observe, attempting to execute the `count` query below will result in an `AnalysisException`. Queries in Structured Streaming are a continuous operation that needs to be scheduled. To start scheduling queries on a stream, we use the `writeStream.start()` operation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "47C12878560C49089D4E4742729D9694",
    "input_collapsed": false
   },
   "outputs": [],
   "source": [
    "// expect this call to fail!\n",
    "val count = webLogStream.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4288478AA31B49928541E3795ED7217F"
   },
   "source": [
    "## What are popular URLs? In what timeframe?\n",
    "\n",
    "Now that we have immediate analytic access to the stream of weblogs we don't need to wait for a day or a month to have a rank of the popular URLs. We can have that information as trends unfold on much shorter windows of time.\n",
    "\n",
    "To define the period of our interest, we create a window over some timestamp. An interesting feature of Structured Streaming is that we can define that window on the timestamp when the data was produced, also known as 'event time' as opposed to the time when the data is processed.\n",
    "\n",
    "Our window definition is of 5 minutes of event data. Given that the TCP Server is replaying the logs in a simulated timeline, the 5 minutes might happen much faster or slower than the clock time. In this way, we can appreciate how Structured Streaming uses the timestamp information in the events to keep track of the event timeline.\n",
    "\n",
    "As we learned from the batch analytics, we should extract the URLs and only select content pages, like `html`, `htm`, or directories. Let's apply that acquired knowledge first before proceeding to define our `window` query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A033C0056A0F478B806143935073E3E4",
    "input_collapsed": false
   },
   "outputs": [],
   "source": [
    "// A regex expression to extract the accessed URL from weblog.request \n",
    "val urlExtractor = \"\"\"^GET (.+) HTTP/\\d.\\d\"\"\".r\n",
    "val allowedExtensions = Set(\".html\",\".htm\", \"\")\n",
    "\n",
    "val contentPageLogs: String => Boolean = url => {\n",
    "  val ext = url.takeRight(5).dropWhile(c => c != '.')\n",
    "  allowedExtensions.contains(ext)\n",
    "}\n",
    "\n",
    "val urlWebLogStream = webLogStream.flatMap{ weblog => \n",
    "  weblog.request match {                                        \n",
    "    case urlExtractor(url) if (contentPageLogs(url)) => Some(weblog.copy(request = url))\n",
    "    case _ => None\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "008F6271D01845028313B5DA01D7E2BB"
   },
   "source": [
    "## Top Content Pages Query\n",
    "We have converted the request to only contain the visited URL and filtered out all non-content pages. \n",
    "We will now define the windowed query to compute the top trending URLs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "00D498E82EBB42DD8EB660C40F15D8F0",
    "input_collapsed": false
   },
   "outputs": [],
   "source": [
    "val rankingURLStream = urlWebLogStream.groupBy($\"request\", window($\"timestamp\", \"5 minutes\", \"1 minute\")).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D43F6DAFECBB416F9B5C73A9D16B6438"
   },
   "source": [
    "## Start the stream processing\n",
    "All the steps we have followed so far have been to define the process that the stream will undergo but no data has been processed yet. \n",
    "\n",
    "To start a Structured Streaming job, we need to specify a `sink` and an `output mode`. \n",
    "These are two new concepts introduced by Structured Streaming.\n",
    "\n",
    "A `sink` defines where we want to materialize the resulting data, like to a file in a file system, to an in-memory table or to another streaming system such as Kafka.\n",
    "The `output mode` defines how we want the results to be delivered: Do we want to see all data every time, only updates or just the new records? \n",
    "\n",
    "These options are given to a `writeStream` operation that creates the streaming query that starts the stream consumption, materializes the computations \n",
    "declared on the query and produces the result to the output `sink`.\n",
    "\n",
    "We will visit all these concepts in detail later on. For now, we will use them empirically and observe the results.\n",
    "\n",
    "For our query, we will use the `memory` `sink` and output mode `complete` to have a fully updated table each time new records are added to the result of keeping track of the URL ranking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B01C2A1937204F6E8F001BF0ABFD0BDB",
    "input_collapsed": false
   },
   "outputs": [],
   "source": [
    "val query = rankingURLStream.writeStream\n",
    "  .queryName(\"urlranks\")\n",
    "  .outputMode(\"complete\")\n",
    "  .format(\"memory\")\n",
    "  .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8FB679DD8DD045E69BEF5DC74FB427E2"
   },
   "source": [
    "### The memory sink outputs the data to a temporary table of the same name given in the queryName option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F9F3A081AA0B4ED188828F2025139163",
    "input_collapsed": false
   },
   "outputs": [],
   "source": [
    "sparkSession.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "95836A339A17498C8A35459222AE3BDF"
   },
   "source": [
    "## Exploring the Data\n",
    "The `memory` `sink` outputs the data to a temporary table of the same name given in the `queryName` option. We can create a `DataFrame` from that table to explore the results of the stream process. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7BDA487B44AD4AFF830A9F7DF469B764",
    "input_collapsed": false
   },
   "outputs": [],
   "source": [
    "val urlRanks = sparkSession.sql(\"select * from urlranks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "20C25E20A8354E0E8F21FDDBE666A6F0"
   },
   "source": [
    "### Before we can see any materialized results, we need to wait for the window to complete.\n",
    "Given that we are accelerating the log timeline on the producer side, after few seconds, we can execute the next command to see the result of the first windows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A7AB127108814D1A9E76FC49E8893140",
    "input_collapsed": false
   },
   "outputs": [],
   "source": [
    "urlRanks.select($\"request\", $\"window\", $\"count\").orderBy(desc(\"count\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "3BAF4A0860EE44548BD2D1D18882F142",
    "input_collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "auto_save_timestamp": "1970-01-01T01:00:00.000Z",
  "customArgs": null,
  "customDeps": null,
  "customImports": null,
  "customLocalRepo": null,
  "customRepos": null,
  "customSparkConf": null,
  "customVars": null,
  "id": "5297db27-a8c6-40dc-b442-d4b6f279e3bb",
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  },
  "name": "streaming_weblogs",
  "sparkNotebook": null,
  "trusted": true,
  "user_save_timestamp": "1970-01-01T01:00:00.000Z"
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
