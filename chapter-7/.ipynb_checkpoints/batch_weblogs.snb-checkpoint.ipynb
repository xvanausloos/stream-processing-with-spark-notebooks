{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A7ECD7E2EBF047A283DBADDCA0BB55E1"
   },
   "source": [
    "# Simple Weblog Analytics - The Batch Way\n",
    "In this notebook, we are going to quickly visit a batch process of a series of weblog files to obtain the top trending pages per day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "4D776A6E557446488681874D1D710705",
    "input_collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "logsDirectory: String = /home/ldi/nasa_dataset_july_1995\n",
       "rawLogs: org.apache.spark.sql.DataFrame = [bytes: bigint, host: string ... 3 more fields]\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// This is the location of the unpackaged files. Update accordingly\n",
    "// You can unpack the provided dataset with:\n",
    "// tar xvf datasets/NASA-weblogs/nasa_dataset_july_1995.tgz -C /tmp/data/\n",
    "val logsDirectory = \"/home/ldi/nasa_dataset_july_1995\"\n",
    "val rawLogs = spark.read.json(logsDirectory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A60148A17B904A72A421640BB8E87474"
   },
   "source": [
    "## We define a schema for the data in the logs\n",
    "Following the formal description of the dataset (at: [NASA-HTTP](http://ita.ee.lbl.gov/html/contrib/NASA-HTTP.html) ), the log is structured as follows:\n",
    "\n",
    ">The logs are an ASCII file with one line per request, with the following columns:\n",
    "- host making the request. A hostname when possible, otherwise the Internet address if the name could not be looked up.\n",
    "- timestamp in the format \"DAY MON DD HH:MM:SS YYYY\", where DAY is the day of the week, MON is the name of the month, DD is the day of the month, HH:MM:SS is the time of day using a 24-hour clock, and YYYY is the year. The timezone is -0400.\n",
    "- request given in quotes.\n",
    "- HTTP reply code.\n",
    "- bytes in the reply.\n",
    "\n",
    "The dataset provided for this exercise offers this data in JSON format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "82F9E2879A1747EF8CC36B5E5D64E701",
    "input_collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import java.sql.Timestamp\n",
       "defined class WebLog\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import java.sql.Timestamp\n",
    "case class WebLog(host:String, \n",
    "                  timestamp: Timestamp, \n",
    "                  request: String, \n",
    "                  http_reply:Int, \n",
    "                  bytes: Long\n",
    "                 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CB0651C3AB3747EF9CB38477A6E59201"
   },
   "source": [
    "## We convert the raw data to structured logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "7AF0F566B58C49F195A3D1E4990BA3E6",
    "input_collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions._\n",
       "import org.apache.spark.sql.types.IntegerType\n",
       "preparedLogs: org.apache.spark.sql.DataFrame = [bytes: bigint, host: string ... 3 more fields]\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.types.IntegerType\n",
    "val preparedLogs = rawLogs.withColumn(\"http_reply\", $\"http_reply\".cast(IntegerType))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "833438CD8CA1471181AC509553B749C2",
    "input_collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------------+----------+--------------------+--------------------+\n",
      "|  bytes|            host|http_reply|             request|           timestamp|\n",
      "+-------+----------------+----------+--------------------+--------------------+\n",
      "|      0| nntp1.reach.com|       304|GET /images/NASA-...|1995-07-13T00:00:...|\n",
      "|  32252|webgate1.mot.com|       200|GET /shuttle/tech...|1995-07-13T00:00:...|\n",
      "|    751|webgate1.mot.com|       200|GET /htbin/cdt_cl...|1995-07-13T00:00:...|\n",
      "|1121554|  204.157.128.52|       200|GET /shuttle/miss...|1995-07-13T00:00:...|\n",
      "|      0| nntp1.reach.com|       304|GET /images/KSC-l...|1995-07-13T00:00:...|\n",
      "+-------+----------------+----------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "ename": "org.apache.spark.sql.AnalysisException",
     "evalue": " Cannot up cast timestamp from string to timestamp.",
     "output_type": "error",
     "traceback": [
      "org.apache.spark.sql.AnalysisException: Cannot up cast timestamp from string to timestamp.",
      "The type path of the target object is:",
      "- field (class: \"java.sql.Timestamp\", name: \"timestamp\")",
      "- root class: \"WebLog\"",
      "You can either add an explicit cast to the input data or choose a higher precision type of the field in the target object",
      "  at org.apache.spark.sql.errors.QueryCompilationErrors$.upCastFailureError(QueryCompilationErrors.scala:154)",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveUpCast$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveUpCast$$fail(Analyzer.scala:3615)",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveUpCast$$anonfun$apply$62$$anonfun$applyOrElse$195.applyOrElse(Analyzer.scala:3645)",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveUpCast$$anonfun$apply$62$$anonfun$applyOrElse$195.applyOrElse(Analyzer.scala:3623)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:481)",
      "  at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:481)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:486)",
      "  at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)",
      "  at scala.collection.Iterator.foreach(Iterator.scala:943)",
      "  at scala.collection.Iterator.foreach$(Iterator.scala:943)",
      "  at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)",
      "  at scala.collection.IterableLike.foreach(IterableLike.scala:74)",
      "  at scala.collection.IterableLike.foreach$(IterableLike.scala:73)",
      "  at scala.collection.AbstractIterable.foreach(Iterable.scala:56)",
      "  at scala.collection.TraversableLike.map(TraversableLike.scala:286)",
      "  at scala.collection.TraversableLike.map$(TraversableLike.scala:279)",
      "  at scala.collection.AbstractTraversable.map(Traversable.scala:108)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:595)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:486)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:486)",
      "  at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)",
      "  at scala.collection.Iterator.foreach(Iterator.scala:943)",
      "  at scala.collection.Iterator.foreach$(Iterator.scala:943)",
      "  at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)",
      "  at scala.collection.IterableLike.foreach(IterableLike.scala:74)",
      "  at scala.collection.IterableLike.foreach$(IterableLike.scala:73)",
      "  at scala.collection.AbstractIterable.foreach(Iterable.scala:56)",
      "  at scala.collection.TraversableLike.map(TraversableLike.scala:286)",
      "  at scala.collection.TraversableLike.map$(TraversableLike.scala:279)",
      "  at scala.collection.AbstractTraversable.map(Traversable.scala:108)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:595)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:486)",
      "  at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$transformExpressionsDownWithPruning$1(QueryPlan.scala:152)",
      "  at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$1(QueryPlan.scala:193)",
      "  at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)",
      "  at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:193)",
      "  at org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:204)",
      "  at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$4(QueryPlan.scala:214)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:323)",
      "  at org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:214)",
      "  at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsDownWithPruning(QueryPlan.scala:152)",
      "  at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsWithPruning(QueryPlan.scala:123)",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveUpCast$$anonfun$apply$62.applyOrElse(Analyzer.scala:3623)",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveUpCast$$anonfun$apply$62.applyOrElse(Analyzer.scala:3619)",
      "  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:138)",
      "  at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)",
      "  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)",
      "  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)",
      "  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)",
      "  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)",
      "  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:30)",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveUpCast$.apply(Analyzer.scala:3619)",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveUpCast$.apply(Analyzer.scala:3609)",
      "  at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:211)",
      "  at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)",
      "  at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)",
      "  at scala.collection.immutable.List.foldLeft(List.scala:91)",
      "  at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:208)",
      "  at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:200)",
      "  at scala.collection.immutable.List.foreach(List.scala:431)",
      "  at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:200)",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:222)",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:218)",
      "  at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:167)",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:218)",
      "  at org.apache.spark.sql.catalyst.encoders.ExpressionEncoder.resolveAndBind(ExpressionEncoder.scala:345)",
      "  at org.apache.spark.sql.Dataset.resolvedEnc$lzycompute(Dataset.scala:239)",
      "  at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$resolvedEnc(Dataset.scala:238)",
      "  at org.apache.spark.sql.Dataset$.apply(Dataset.scala:82)",
      "  at org.apache.spark.sql.Dataset.as(Dataset.scala:464)",
      "  ... 40 elided",
      ""
     ]
    }
   ],
   "source": [
    "preparedLogs.show(5)\n",
    "val weblogs = preparedLogs.as[WebLog] //convert DF preparedLogs to a dataset of given type (Weblog)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "9313FDE7CE7548B180AE2A47CDBED56E",
    "input_collapsed": false
   },
   "source": [
    "## Now, we have the data in a structured format and we can start asking the questions that interest us.\n",
    "We have imported the data and transformed it using a known schema.  We can use this 'structured' data to create queries that provide insights in the behavior of the users. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "144FB0F8C3AE42328435A2A8F3C150AB"
   },
   "source": [
    "### As a first step, we would like to know how many records are contained in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FBE9E30417204C9D83E8F1FE65AB6471",
    "input_collapsed": false
   },
   "outputs": [],
   "source": [
    "val recordCount = weblogs.count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BEA0D41E58324A479D2B91093864E6C7"
   },
   "source": [
    "### A common question would be, what was the most popular URL per day?\n",
    "We first reduce the timestamp to the day of the year. We then group by this new 'day of year' column and the request url and we count over this aggregate. We finally order using descending order to get this top URLs first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B65C7756F4EA47388884AB0DC4619CE8",
    "input_collapsed": false
   },
   "outputs": [],
   "source": [
    "val topDailyURLs = weblogs.withColumn(\"dayOfMonth\", dayofmonth($\"timestamp\"))\n",
    "                          .select($\"request\", $\"dayOfMonth\")\n",
    "                          .groupBy($\"dayOfMonth\", $\"request\")\n",
    "                          .agg(count($\"request\").alias(\"count\"))\n",
    "                          .orderBy(desc(\"count\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "03E60D724B504A918E10873AEF0F0CF1",
    "input_collapsed": false
   },
   "outputs": [],
   "source": [
    "topDailyURLs.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F1CB4530E62149F08D39256D53BA1C07",
    "input_collapsed": false,
    "presentation": {
     "pivot_chart_state": "{\n  \"hiddenAttributes\": [],\n  \"menuLimit\": 200,\n  \"cols\": [],\n  \"rows\": [],\n  \"vals\": [],\n  \"exclusions\": {},\n  \"inclusions\": {},\n  \"unusedAttrsVertical\": 85,\n  \"autoSortUnusedAttrs\": false,\n  \"inclusionsInfo\": {},\n  \"aggregatorName\": \"Count\",\n  \"rendererName\": \"Table\"\n}",
     "tabs_state": "{\n  \"tab_id\": \"#tab2096092733-0\"\n}"
    }
   },
   "outputs": [],
   "source": [
    "topDailyURLs.take(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4F7AB40ADF1D4EFF9166CDB63A0DC77E"
   },
   "source": [
    "### Top hits are all images. What now?\n",
    "It's not unusual to see that the top URLs are images commonly used across a site.\n",
    "\n",
    "Our true interest lies in the content pages generating most traffic. To find those, we first filter on `html` content \n",
    "and then proceed to apply the top aggregation we just learned.\n",
    "\n",
    "The request field is a quoted sequence of `[HTTP_VERB] URL [HTTP_VERSION]`. We will extract the url and preserve only those ending in `.html`, `.htm` or no extension (directories). This is a simplification for the purpose of the exercise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7191956637C34DF28D24D2947E6999A8",
    "input_collapsed": false
   },
   "outputs": [],
   "source": [
    "val urlExtractor = \"\"\"^GET (.+) HTTP/\\d.\\d\"\"\".r\n",
    "val allowedExtensions = Set(\".html\",\".htm\", \"\")\n",
    "val contentPageLogs = weblogs.filter {log => \n",
    "  log.request match {                                        \n",
    "    case urlExtractor(url) => \n",
    "      val ext = url.takeRight(5).dropWhile(c => c != '.')\n",
    "      allowedExtensions.contains(ext)\n",
    "    case _ => false \n",
    "  }\n",
    "}\n",
    "                                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "844604F254BF4450B2D60F09A766C444",
    "input_collapsed": false,
    "presentation": {
     "pivot_chart_state": "{\n  \"hiddenAttributes\": [],\n  \"menuLimit\": 200,\n  \"cols\": [],\n  \"rows\": [],\n  \"vals\": [],\n  \"exclusions\": {},\n  \"inclusions\": {},\n  \"unusedAttrsVertical\": 85,\n  \"autoSortUnusedAttrs\": false,\n  \"inclusionsInfo\": {},\n  \"aggregatorName\": \"Count\",\n  \"rendererName\": \"Table\"\n}",
     "tabs_state": "{\n  \"tab_id\": \"#tab1029193174-0\"\n}"
    }
   },
   "outputs": [],
   "source": [
    "val topContentPages = contentPageLogs.withColumn(\"dayOfMonth\", dayofmonth($\"timestamp\"))\n",
    "                          .select($\"request\", $\"dayOfMonth\")\n",
    "                          .groupBy($\"dayOfMonth\", $\"request\")\n",
    "                          .agg(count($\"request\").alias(\"count\"))\n",
    "                          .orderBy(desc(\"count\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "392A9C52AEC34D9F8EF709F5B407F0E1",
    "input_collapsed": false
   },
   "outputs": [],
   "source": [
    "topContentPages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "995C97DC2E9F4DBA95A832588B147843"
   },
   "source": [
    "We can see that the most popular page that month was `liftoff.html ` corresponding to the coverage of the launch of the Discovery shuttle, as documented on the NASA archives: https://www.nasa.gov/mission_pages/shuttle/shuttlemissions/archives/sts-70.html.\n",
    "\n",
    "It's closely followed by `countdown/` the days prior ot the launch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8DBF5B380B654300B6064149A1B63ABC",
    "input_collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "auto_save_timestamp": "1970-01-01T01:00:00.000Z",
  "customArgs": null,
  "customDeps": null,
  "customImports": null,
  "customLocalRepo": null,
  "customRepos": null,
  "customSparkConf": null,
  "customVars": null,
  "id": "c8749075-25ae-48aa-a102-495a26a92d54",
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  },
  "name": "batch_weblogs",
  "sparkNotebook": null,
  "trusted": true,
  "user_save_timestamp": "1970-01-01T01:00:00.000Z"
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
